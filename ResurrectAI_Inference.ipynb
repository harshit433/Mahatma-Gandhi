{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Unsloth library"
      ],
      "metadata": {
        "id": "JEiUh70OJHRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the Unsloth library\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Installing the other libraries required by unsloth to run\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes triton"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GJswWnol1htB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "OjelLnAfIubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "2IK2JmhJInwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model for inferencing"
      ],
      "metadata": {
        "id": "pvyzioS42gek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Maximum number of token allowed in a sequence\n",
        "\n",
        "# Loading the model for running inference from ressurectAI available at https://huggingface.co/ressurectAI/base\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"ressurectAI/base\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        ")\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"Person\", \"assistant\": \"Gandhi\"},\n",
        "    chat_template=\"chatml\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "Eq9r0mitjqa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate response\n",
        "def send_message(input):\n",
        "  messages = [\n",
        "    {\"from\": \"Person\", \"value\": \"Gandhi ji, {}\".format(input)},\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=False,\n",
        "      return_tensors=\"pt\",\n",
        "  ).to(\"cuda\")\n",
        "  result = model.generate(input_ids=inputs,pad_token_id = tokenizer.pad_token_id , max_new_tokens=1024,use_cache=True)\n",
        "  response = str(tokenizer.batch_decode(result)[0].split('assistant\\n')[1].replace('<|im_end|>','').strip())\n",
        "  return response"
      ],
      "metadata": {
        "id": "qqzAT-qtli4m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interact with the model"
      ],
      "metadata": {
        "id": "sil80PpQ2bWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  user_input = input('User: ')\n",
        "  print('Gandhiji: ',send_message(user_input))\n",
        "  if input == 'exit':\n",
        "    break"
      ],
      "metadata": {
        "id": "2CUFydrONtI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}